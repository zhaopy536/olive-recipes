{
    "copies": [
        {
            "src": "../../deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B/aitk/deepseek_qnn_config.json",
            "dst": "llama3_2_qnn_config.json",
            "replacements": [
                {
                    "find": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                    "replace": "meta-llama/Llama-3.2-1B-Instruct"
                },
                {
                    "find": "model/deepseek",
                    "replace": "model/llama3_2"
                }
            ]
        },
        {
            "src": "../../deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B/aitk/deepseek_vitis_ai_config.json",
            "dst": "llama3_2_vitis_ai_config.json",
            "replacements": [
                {
                    "find": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                    "replace": "meta-llama/Llama-3.2-1B-Instruct"
                },
                {
                    "find": "model/deepseek",
                    "replace": "model/llama3_2"
                }
            ]
        },
        {
            "src": "../../deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B/aitk/deepseek_trtrtx_config.json",
            "dst": "llama3_2_trtrtx_config.json",
            "replacements": [
                {
                    "find": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                    "replace": "meta-llama/Llama-3.2-1B-Instruct"
                },
                {
                    "find": "model/deepseek",
                    "replace": "model/llama3_2"
                }
            ]
        },
        {
            "src": "../../deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B/aitk/deepseek_trtrtx_config.json.config",
            "dst": "llama3_2_trtrtx_config.json.config",
            "replacements": []
        },
        {
            "src": "../../deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B/aitk/deepseek_dml_config.json",
            "dst": "llama3_2_dml_config.json",
            "replacements": [
                {
                    "find": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                    "replace": "meta-llama/Llama-3.2-1B-Instruct"
                },
                {
                    "find": "model/deepseek",
                    "replace": "model/llama3_2"
                }
            ]
        },
        {
            "src": "../../deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B/aitk/deepseek_dml_config.json.config",
            "dst": "llama3_2_dml_config.json.config",
            "replacements": []
        },
        {
            "src": "../../deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B/aitk/README.md",
            "dst": "README.md",
            "replacements": [
                {
                    "find": "# DeepSeek-R1-Distill-Qwen-1.5B Model Optimization",
                    "replace": "# Llama-3.2-1B-Instruct Model Optimization"
                },
                {
                    "find": "[DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)",
                    "replace": "[Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)"
                },
                {
                    "find": "> ⚠️ If got 6033 error, replace `genai_config.json` in `./model` folder",
                    "replace": ""
                }
            ]
        },
        {
            "src": "../../deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B/aitk/requirements.txt",
            "dst": "requirements.txt",
            "replacements": []
        },
        {
            "src": "../../deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B/aitk/inference_sample.ipynb",
            "dst": "inference_sample.ipynb",
            "replacements": [
                {
                    "find": "<｜User｜>{input}<｜Assistant｜><think>",
                    "replace": "<|start_header_id|>user<|end_header_id|>\\\\n{input}<|start_header_id|>assistant<|end_header_id|>\\\\n"
                }
            ]
        },
        {
            "src": "../../intel-bert-base-uncased-mrpc/aitk/winml.py",
            "dst": "winml.py"
        },
        {
            "src": "llama3_2_dml_config.json",
            "dst": "llama3_2_migraphx_config.json",
            "replacements": [
                {
                    "find": "DmlExecutionProvider",
                    "replace": "MIGraphXExecutionProvider"
                }
            ]
        },
        {
            "src": "llama3_2_dml_config.json.config",
            "dst": "llama3_2_migraphx_config.json.config",
            "replacements": [
                {
                    "find": "DmlExecutionProvider",
                    "replace": "MIGraphXExecutionProvider"
                },
                {
                    "find": "DirectML",
                    "replace": "AMD GPU"
                }
            ]
        }
    ]
}
